{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music recommender system\n",
    "\n",
    "One of the most used machine learning algorithms is recommendation systems. A **recommender** (or recommendation) **system** (or engine) is a filtering system which aim is to predict a rating or preference a user would give to an item, eg. a film, a product, a song, etc.\n",
    "\n",
    "Which type of recommender can we have?   \n",
    "\n",
    "There are two main types of recommender systems: \n",
    "- Content-based filters\n",
    "- Collaborative filters\n",
    "  \n",
    "> Content-based filters predicts what a user likes based on what that particular user has liked in the past. On the other hand, collaborative-based filters predict what a user like based on what other users, that are similar to that particular user, have liked.\n",
    "\n",
    "### 1) Content-based filters\n",
    "\n",
    "Recommendations done using content-based recommenders can be seen as a user-specific classification problem. This classifier learns the user's likes and dislikes from the features of the song.\n",
    "\n",
    "The most straightforward approach is **keyword matching**.\n",
    "\n",
    "In a few words, the idea behind is to extract meaningful keywords present in a song description a user likes, search for the keywords in other song descriptions to estimate similarities among them, and based on that, recommend those songs to the user.\n",
    "\n",
    "*How is this performed?*\n",
    "\n",
    "In our case, because we are working with text and words, **Term Frequency-Inverse Document Frequency (TF-IDF)** can be used for this matching process.\n",
    "  \n",
    "We'll go through the steps for generating a **content-based** music recommender system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries\n",
    "\n",
    "First, we'll import all the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already used the **TF-IDF score before** when performing Twitter sentiment analysis. \n",
    "\n",
    "Likewise, we are going to use TfidfVectorizer from the Scikit-learn package again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So imagine that we have the [following dataset](https://www.kaggle.com/mousehead/songlyrics/data#). \n",
    "\n",
    "This dataset contains name, artist, and lyrics for *57650 songs in English*. The data has been acquired from LyricsFreak through scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = pd.read_csv('songdata.csv', nrows=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\r\\nTouch me gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\r\\nTouch me gen...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the dataset being so big, we are going to resample only 5000 random songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# songs = songs.sample(n=5000).drop('link', axis=1).reset_index(drop=True)\n",
    "songs = songs.drop('link', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice also the presence of `\\n` in the text, so we are going to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs['text'] = songs['text'].str.replace(r'\\n', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Look at her face, it's a wonderful face  And i...\n",
       "1       Take it easy with me, please  \\rTouch me gentl...\n",
       "2       I'll never know why I had to go  Why I had to ...\n",
       "3       Making somebody happy is a question of give an...\n",
       "4       Making somebody happy is a question of give an...\n",
       "                              ...                        \n",
       "4995    You won't take my love for tender  \\rYou can p...\n",
       "4996    I've looked at it every way I can  \\rFrom unde...\n",
       "4997    I won't walk with my head bowed  \\r(Be on) Bey...\n",
       "4998    Dressed up like a dog's dinner  \\rButter would...\n",
       "4999    Now there's newsprint all over your face  \\rWe...\n",
       "Name: text, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we use TF-IDF vectorizerthat calculates the TF-IDF score for each song lyric, word-by-word. \n",
    "\n",
    "Here, we pay particular attention to the arguments we can specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word', stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_matrix = tfidf.fit_transform(songs['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\n",
      "  (0, 13612)\t0.14426211585703372\n",
      "  (0, 18535)\t0.08193734815347754\n",
      "  (0, 18198)\t0.12377179609942188\n",
      "  (0, 8774)\t0.13662249282824912\n",
      "  (0, 20023)\t0.11702773810586717\n",
      "  (0, 10534)\t0.053061945396677815\n",
      "  (0, 8132)\t0.09211566628984386\n",
      "  (0, 17289)\t0.237566856563542\n",
      "  (0, 8617)\t0.16377780427488617\n",
      "  (0, 13054)\t0.14834215090454939\n",
      "  (0, 20018)\t0.09380471149390261\n",
      "  (0, 10268)\t0.2655820946681044\n",
      "  (0, 1981)\t0.19507670721087733\n",
      "  (0, 1634)\t0.18059738027083114\n",
      "  (0, 6712)\t0.2196470434421451\n",
      "  (0, 6570)\t0.1391315848631338\n",
      "  (0, 10952)\t0.2090553517993387\n",
      "  (0, 7576)\t0.3292555462476679\n",
      "  (0, 9920)\t0.4240399935128574\n",
      "  (0, 9772)\t0.18814964017738572\n",
      "  (0, 6588)\t0.19423747679701758\n",
      "  (0, 10743)\t0.13859327906897498\n",
      "  (0, 15964)\t0.14785705566619378\n",
      "  (0, 16778)\t0.16051052032357432\n",
      "  (0, 20158)\t0.06193913808495802\n",
      "  :\t:\n",
      "  (4999, 10659)\t0.06471177496103261\n",
      "  (4999, 7698)\t0.05844985120541044\n",
      "  (4999, 11221)\t0.0632354956813946\n",
      "  (4999, 14606)\t0.07529524589665534\n",
      "  (4999, 7532)\t0.06965011383607916\n",
      "  (4999, 8292)\t0.2629841059899671\n",
      "  (4999, 19180)\t0.06925571185996914\n",
      "  (4999, 15728)\t0.11383730461350798\n",
      "  (4999, 3598)\t0.062284973810334494\n",
      "  (4999, 11344)\t0.3721138166919206\n",
      "  (4999, 10267)\t0.05667180043111619\n",
      "  (4999, 14044)\t0.08333880680261066\n",
      "  (4999, 18571)\t0.05828749054120552\n",
      "  (4999, 19154)\t0.056059862889377254\n",
      "  (4999, 18536)\t0.046805096413759475\n",
      "  (4999, 10695)\t0.030088567271135697\n",
      "  (4999, 12239)\t0.05602455615330573\n",
      "  (4999, 18193)\t0.06459345938306416\n",
      "  (4999, 6351)\t0.045108423491403433\n",
      "  (4999, 10427)\t0.06303828713076107\n",
      "  (4999, 10534)\t0.03288017688759532\n",
      "  (4999, 9920)\t0.06568977960517096\n",
      "  (4999, 9772)\t0.058294069357257124\n",
      "  (4999, 6368)\t0.10908139249905864\n",
      "  (4999, 10621)\t0.192541320189696\n"
     ]
    }
   ],
   "source": [
    "# print(lyrics_matrix)\n",
    "matrix_arr = np.array(lyrics_matrix)\n",
    "\n",
    "print('==================================')\n",
    "print(matrix_arr)\n",
    "# print(tfidf.vocabulary_) # dictionary of words and their index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How do we use this matrix for a recommendation?* \n",
    "\n",
    "We now need to calculate the similarity of one lyric to another. We are going to use **cosine similarity**.\n",
    "\n",
    "We want to calculate the cosine similarity of each item with every other item in the dataset. So we just pass the lyrics_matrix as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.00169743 0.00936841 ... 0.03331108 0.03366429 0.08968344]\n",
      " [0.00169743 1.         0.0044533  ... 0.00275294 0.01246284 0.00367714]\n",
      " [0.00936841 0.0044533  1.         ... 0.00674108 0.00608736 0.0225969 ]\n",
      " ...\n",
      " [0.03331108 0.00275294 0.00674108 ... 1.         0.01257674 0.01331615]\n",
      " [0.03366429 0.01246284 0.00608736 ... 0.01257674 1.         0.07131912]\n",
      " [0.08968344 0.00367714 0.0225969  ... 0.01331615 0.07131912 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "cosine_similarities = cosine_similarity(lyrics_matrix) \n",
    "print(cosine_similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x20849 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 250920 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "XA must be a 2-dimensional array.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m distance_matrix \u001b[39m=\u001b[39m distance\u001b[39m.\u001b[39;49mcdist(lyrics_matrix, lyrics_matrix, metric\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39meuclidean\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(distance_matrix)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\spatial\\distance.py:2916\u001b[0m, in \u001b[0;36mcdist\u001b[1;34m(XA, XB, metric, out, **kwargs)\u001b[0m\n\u001b[0;32m   2913\u001b[0m sB \u001b[39m=\u001b[39m XB\u001b[39m.\u001b[39mshape\n\u001b[0;32m   2915\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(s) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m-> 2916\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mXA must be a 2-dimensional array.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   2917\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(sB) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m   2918\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mXB must be a 2-dimensional array.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: XA must be a 2-dimensional array."
     ]
    }
   ],
   "source": [
    "distance_matrix = distance.cdist(lyrics_matrix, lyrics_matrix, metric='euclidean')\n",
    "print(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_matrix, lyrics_matrix, metric='euclidean')\n",
    "print(distance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we get the similarities, we'll store in a dictionary the names of the 50  most similar songs for each song in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cosine_similarities)):\n",
    "    # Now we'll sort each element in cosine_similarities and get the indexes of the songs. \n",
    "    similar_indices = cosine_similarities[i].argsort()[:-50:-1] \n",
    "    # After that, we'll store in similarities each name of the 50 most similar songs.\n",
    "    # Except the first one that is the same song.\n",
    "    similarities[songs['song'].iloc[i]] = [(cosine_similarities[i][x], songs['song'][x], songs['artist'][x]) for x in similar_indices][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, all the magic happens. We can use that similarity scores to access the most similar items and give a recommendation.\n",
    "\n",
    "For that, we'll define our Content based recommender class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentBasedRecommender:\n",
    "    def __init__(self, matrix):\n",
    "        self.matrix_similar = matrix\n",
    "\n",
    "    def _print_message(self, song, recom_song):\n",
    "        rec_items = len(recom_song)\n",
    "        \n",
    "        print(f'The {rec_items} recommended songs for {song} are:')\n",
    "        for i in range(rec_items):\n",
    "            print(f\"Number {i+1}:\")\n",
    "            print(f\"{recom_song[i][1]} by {recom_song[i][2]} with {round(recom_song[i][0], 3)} similarity score\") \n",
    "            print(\"--------------------\")\n",
    "        \n",
    "    def recommend(self, recommendation):\n",
    "        # Get song to find recommendations for\n",
    "        song = recommendation['song']\n",
    "        # Get number of songs to recommend\n",
    "        number_songs = recommendation['number_songs']\n",
    "        # Get the number of songs most similars from matrix similarities\n",
    "        recom_song = self.matrix_similar[song][:number_songs]\n",
    "        # print each item\n",
    "        self._print_message(song=song, recom_song=recom_song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instantiate class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommedations = ContentBasedRecommender(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we are ready to pick a song from the dataset and make a recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation = {\n",
    "    \"song\": songs['song'].iloc[10],\n",
    "    \"number_songs\": 4 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 4 recommended songs for Dance are:\n",
      "Number 1:\n",
      "Life Is A Dance by Chaka Khan with 0.568 similarity score\n",
      "--------------------\n",
      "Number 2:\n",
      "Do You Wanna Dance? by Beach Boys with 0.504 similarity score\n",
      "--------------------\n",
      "Number 3:\n",
      "Do You Wanna Dance? by Cliff Richard with 0.472 similarity score\n",
      "--------------------\n",
      "Number 4:\n",
      "Let's Dance by Chris Rea with 0.445 similarity score\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "recommedations.recommend(recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can pick another random song and recommend again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation2 = {\n",
    "    \"song\": songs['song'].iloc[120],\n",
    "    \"number_songs\": 4 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 4 recommended songs for Life Is A Flower are:\n",
      "Number 1:\n",
      "World Where You Live by Crowded House with 0.386 similarity score\n",
      "--------------------\n",
      "Number 2:\n",
      "All Over The World by Arlo Guthrie with 0.334 similarity score\n",
      "--------------------\n",
      "Number 3:\n",
      "The World Is Mine by David Guetta with 0.313 similarity score\n",
      "--------------------\n",
      "Number 4:\n",
      "World Of Two by Cake with 0.276 similarity score\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "recommedations.recommend(recommendation2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'This is the first document is.',\n",
    "     'This document is the second document.'\n",
    " ]\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word', stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t0.5749618667993135\n",
      "  (1, 0)\t0.8181802073667197\n",
      "{'document': 0, 'second': 1}\n"
     ]
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X)\n",
    "print(vectorizer.vocabulary_) # dictionary of words and their index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'sixty', 'whose', 'top', 'anything', 'our', 'on', 'find', 'about', 'the', 'among', 'over', 'none', 'whereupon', 'interest', 'became', 'into', 'amongst', 'thereafter', 'by', 'has', 'elsewhere', 'further', 'four', 'anyhow', 'below', 'should', 'is', 'together', 're', 'six', 'fifteen', 'he', 'third', 'must', 'in', 'no', 'behind', 'done', 'whereas', 'would', 'whenever', 'yourselves', 'detail', 'name', 'him', 'please', 'fill', 'though', 'already', 'former', 'twenty', 'again', 'side', 'could', 'often', 'cant', 'because', 'against', 'its', 'hasnt', 'much', 'nothing', 'several', 'being', 'less', 'between', 'yourself', 'show', 'what', 'but', 'noone', 'few', 'ie', 'thin', 'found', 'whom', 'and', 'also', 'etc', 'am', 'eg', 'moreover', 'full', 'such', 'she', 'wherever', 'whereby', 'been', 'ten', 'any', 'whether', 'they', 'somewhere', 'more', 'serious', 'part', 'everywhere', 'almost', 'namely', 'indeed', 'onto', 'have', 'five', 'whole', 'con', 'hundred', 'next', 'can', 'which', 'this', 'that', 'may', 'get', 'see', 'eleven', 'mine', 'when', 'until', 'within', 'not', 'sometime', 'those', 'rather', 'whoever', 'afterwards', 'per', 'except', 'ltd', 'due', 'thereby', 'something', 'if', 'whence', 'are', 'give', 'twelve', 'everyone', 'every', 'thru', 'describe', 'well', 'even', 'eight', 'mostly', 'be', 'where', 'one', 'call', 'cannot', 'without', 'un', 'some', 'me', 'latterly', 'mill', 'same', 'towards', 'thence', 'before', 'nor', 'thereupon', 'either', 'whereafter', 'very', 'becoming', 'go', 'couldnt', 'hereafter', 'yet', 'amoungst', 'alone', 'then', 'sincere', 'or', 'toward', 'beside', 'least', 'whatever', 'keep', 'my', 'you', 'itself', 'else', 'hereby', 'themselves', 'although', 'throughout', 'neither', 'as', 'amount', 'around', 'wherein', 'otherwise', 'why', 'still', 'we', 'most', 'own', 'seemed', 'a', 'their', 'inc', 'so', 'of', 'cry', 'herein', 'once', 'up', 'therein', 'here', 'nowhere', 'along', 'de', 'nobody', 'ours', 'besides', 'anyone', 'beforehand', 'an', 'co', 'us', 'her', 'ourselves', 'nevertheless', 'move', 'another', 'forty', 'first', 'last', 'many', 'under', 'ever', 'nine', 'each', 'who', 'your', 'had', 'made', 'his', 'off', 'through', 'everything', 'bottom', 'seems', 'how', 'empty', 'it', 'therefore', 'was', 'since', 'enough', 'seem', 'seeming', 'after', 'there', 'however', 'with', 'above', 'all', 'himself', 'too', 'hereupon', 'never', 'system', 'i', 'put', 'only', 'three', 'beyond', 'hence', 'than', 'latter', 'herself', 'during', 'across', 'them', 'both', 'always', 'these', 'fire', 'hers', 'sometimes', 'at', 'anyway', 'take', 'do', 'meanwhile', 'other', 'others', 'somehow', 'whither', 'thick', 'myself', 'becomes', 'were', 'now', 'front', 'might', 'formerly', 'fifty', 'become', 'will', 'two', 'down', 'from', 'while', 'yours', 'bill', 'to', 'anywhere', 'for', 'via', 'perhaps', 'someone', 'thus', 'upon', 'back', 'out'})\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "318f2c383d48220c821fefd227250c35bb828f95042f5889a5ad1704b1baf79c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
